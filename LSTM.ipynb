{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "# from torchtext.legacy import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "seed = 1\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# steup using GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# setuo random seed\n",
    "torch.manual_seed(seed)\n",
    "time_step = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment_24h</th>\n",
       "      <th>DXYUSD_Gain_Rate</th>\n",
       "      <th>World_Index_Gain_Rate</th>\n",
       "      <th>GoldUSD_Gain_Rate</th>\n",
       "      <th>Silver_Gain_Rate</th>\n",
       "      <th>DBCCommodity_Gain_Rate</th>\n",
       "      <th>DJCI_Gain_Rate</th>\n",
       "      <th>CrudeOil_Gain_Rate</th>\n",
       "      <th>SPY_Gain_Rate</th>\n",
       "      <th>VIX_Gain_Rate</th>\n",
       "      <th>TNXTbill_Gain_Rate</th>\n",
       "      <th>BTC_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188838</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>0.006341</td>\n",
       "      <td>0.006341</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.019418</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>117.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.169673</td>\n",
       "      <td>-0.005234</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>-0.014732</td>\n",
       "      <td>-0.014732</td>\n",
       "      <td>-0.007181</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>-0.010167</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.013858</td>\n",
       "      <td>0.014537</td>\n",
       "      <td>103.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131704</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038969</td>\n",
       "      <td>-0.038969</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.002442</td>\n",
       "      <td>-0.022024</td>\n",
       "      <td>-0.006590</td>\n",
       "      <td>0.043948</td>\n",
       "      <td>-0.011460</td>\n",
       "      <td>91.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128927</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>0.029350</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>-0.061464</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>118.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.157061</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.012230</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>-0.000804</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>-0.005418</td>\n",
       "      <td>0.076829</td>\n",
       "      <td>106.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment_24h  DXYUSD_Gain_Rate  World_Index_Gain_Rate  GoldUSD_Gain_Rate  \\\n",
       "0       0.188838          0.001341               0.003931           0.006341   \n",
       "1       0.169673         -0.005234               0.007311          -0.014732   \n",
       "2       0.131704         -0.000857               0.000000          -0.038969   \n",
       "3       0.128927          0.007229               0.004812           0.009762   \n",
       "4       0.157061         -0.001460              -0.012230           0.009474   \n",
       "\n",
       "   Silver_Gain_Rate  DBCCommodity_Gain_Rate  DJCI_Gain_Rate  \\\n",
       "0          0.006341                0.008765        0.019090   \n",
       "1         -0.014732               -0.007181       -0.001202   \n",
       "2         -0.038969               -0.000387       -0.002442   \n",
       "3          0.009762                0.008491        0.029350   \n",
       "4          0.009474                0.001896       -0.000804   \n",
       "\n",
       "   CrudeOil_Gain_Rate  SPY_Gain_Rate  VIX_Gain_Rate  TNXTbill_Gain_Rate  \\\n",
       "0            0.019418       0.003971      -0.000729            0.003007   \n",
       "1           -0.010167       0.002574      -0.013858            0.014537   \n",
       "2           -0.022024      -0.006590       0.043948           -0.011460   \n",
       "3            0.032857       0.006743      -0.061464           -0.007304   \n",
       "4            0.017236       0.001427      -0.005418            0.076829   \n",
       "\n",
       "   BTC_Price  \n",
       "0     117.00  \n",
       "1     103.43  \n",
       "2      91.01  \n",
       "3     118.33  \n",
       "4     106.40  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load FinTech data\n",
    "df = pd.read_csv('../data/FinTech.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1912, 11), (1912, 1))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide the data set into inputs and outputs\n",
    "df_input = df.drop(['BTC_Price'], axis=1).values\n",
    "df_ouput = df[['BTC_Price']].values\n",
    "\n",
    "n_samples, n_features = df_input.shape\n",
    "df_input.shape, df_ouput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized data\n",
    "X_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "X_scaler = X_scaler.fit(df_input)\n",
    "df_input = X_scaler.transform(df_input)\n",
    "# X_test = X_scaler.transform(X_test)\n",
    "\n",
    "Y_scaler = Y_scaler.fit(df_ouput)\n",
    "df_ouput = Y_scaler.transform(df_ouput)\n",
    "# Y_test = Y_scaler.transform(Y_test)\n",
    "# X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1906, 7, 11]), torch.Size([1906, 1]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# built timeseries data for LSTM\n",
    "X_data = np.zeros([n_samples-time_step+1, time_step, n_features])\n",
    "Y_data = torch.Tensor(df_ouput[time_step-1:, :])\n",
    "n_samples = X_data.shape[0]\n",
    "X_data.shape, Y_data.shape\n",
    "\n",
    "for i in range(n_samples):\n",
    "    X_data[i, :, :] = df_input[i:i+time_step, :]\n",
    "    \n",
    "X_data = torch.Tensor(X_data)\n",
    "X_data.shape, Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.25, random_state=1)\n",
    "# make DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size, shuffle=False)\n",
    "# for bath_id, batch_data in enumerate(train_loader):\n",
    "#     print(bath_id, batch_data[0].shape)   # trian_data\n",
    "#     print(bath_id, batch_data[1].shape)   # test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongShortTermMemory(nn.Module):\n",
    "    def __init__(self, timestep, n_features, hidden_size, n_layers):\n",
    "        super(LongShortTermMemory, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.timestep = timestep\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # built LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=timestep, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            bias=True,\n",
    "                            batch_first=False,\n",
    "                            dropout=0.5,\n",
    "                            bidirectional=False\n",
    "                           )\n",
    "        self.num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size*self.num_directions, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # attention net parameter\n",
    "        self.atten_w = nn.Parameter(torch.Tensor(hidden_size*self.num_directions, hidden_size*self.num_directions))\n",
    "        self.atten_u = nn.Parameter(torch.Tensor(hidden_size*self.num_directions, 1))\n",
    "        \n",
    "        # init parameters\n",
    "        nn.init.uniform_(self.atten_w, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.atten_u, -0.1, 0.1)\n",
    "        \n",
    "    def attention_net(self, lstm_output, hidden_state):\n",
    "#         u: [batch_size, num_features, 2 * num_hiddens]\n",
    "        u = torch.relu(torch.matmul(lstm_output, self.atten_w))\n",
    "        \n",
    "        attention = torch.matmul(u, self.atten_u)\n",
    "       # attention: [batch_size, attention, 1]\n",
    "        attention_weight = F.softmax(attention, dim=1)\n",
    "       # att_score: [batch_size, n_features, 1]\n",
    "        context = lstm_output * attention_weight\n",
    "        # context: [bath_size, hidden_size*self.num_directions]\n",
    "        context = torch.sum(context, dim=1)\n",
    "\n",
    "        return context, attention_weight\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(2,0,1)                  #[batch_size, timestep, n_features] -> [n_features, batch_size, timestep]\n",
    "        \n",
    "        # output: [n_features, batch_size, hidden_size * num_directions], hidden_state: [num_directions * num_layers, batch, hidden_size]\n",
    "        output, (hidden_state, cell_state) = self.lstm(x)\n",
    "#         print(\"LSTM_out:\", output.shape, hidden_state.shape, cell_state.shape)\n",
    "        output = output.permute(1, 0, 2)                  #[batch_size, n_features, hidden_size]\n",
    "#         query = self.dropout(output)\n",
    "        attn_output, attention_weight = self.attention_net(output, hidden_state)\n",
    "    \n",
    "#         print(\"fc_input:\", attn_output.shape)\n",
    "        logit = self.fc(attn_output)\n",
    "#         print(\"logit:\", logit.shape)\n",
    "        logit = self.dropout(logit)\n",
    "    \n",
    "        return logit, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Epoch Time: 0.0m 0.14s\n",
      "\tTrain Loss: 0.806613 | Train Acc: 13.26372%\n",
      "\t Val. Loss: 0.443042 |  Val. Acc: 58.37578%\n",
      "Epoch: 11 | Epoch Time: 0.0m 0.12s\n",
      "\tTrain Loss: 0.371108 | Train Acc: 61.10159%\n",
      "\t Val. Loss: 0.255465 |  Val. Acc: 75.97686%\n",
      "Epoch: 21 | Epoch Time: 0.0m 0.12s\n",
      "\tTrain Loss: 0.340900 | Train Acc: 63.35112%\n",
      "\t Val. Loss: 0.220810 |  Val. Acc: 79.21442%\n",
      "Epoch: 31 | Epoch Time: 0.0m 0.12s\n",
      "\tTrain Loss: 0.296224 | Train Acc: 69.29164%\n",
      "\t Val. Loss: 0.205755 |  Val. Acc: 80.51293%\n",
      "Epoch: 41 | Epoch Time: 0.0m 0.11s\n",
      "\tTrain Loss: 0.278187 | Train Acc: 71.22802%\n",
      "\t Val. Loss: 0.198906 |  Val. Acc: 81.09850%\n",
      "Epoch: 51 | Epoch Time: 0.0m 0.10s\n",
      "\tTrain Loss: 0.275303 | Train Acc: 71.07424%\n",
      "\t Val. Loss: 0.187694 |  Val. Acc: 82.23489%\n",
      "Epoch: 61 | Epoch Time: 0.0m 0.10s\n",
      "\tTrain Loss: 0.300140 | Train Acc: 68.68736%\n",
      "\t Val. Loss: 0.185539 |  Val. Acc: 82.51055%\n",
      "Epoch: 71 | Epoch Time: 0.0m 0.11s\n",
      "\tTrain Loss: 0.269542 | Train Acc: 72.13718%\n",
      "\t Val. Loss: 0.193190 |  Val. Acc: 81.84783%\n",
      "Epoch: 81 | Epoch Time: 0.0m 0.12s\n",
      "\tTrain Loss: 0.263147 | Train Acc: 72.36473%\n",
      "\t Val. Loss: 0.197981 |  Val. Acc: 81.42931%\n",
      "Epoch: 91 | Epoch Time: 0.0m 0.11s\n",
      "\tTrain Loss: 0.378592 | Train Acc: 60.13303%\n",
      "\t Val. Loss: 0.223994 |  Val. Acc: 78.90845%\n",
      "Epoch: 101 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.311803 | Train Acc: 66.92153%\n",
      "\t Val. Loss: 0.187500 |  Val. Acc: 82.20326%\n",
      "Epoch: 111 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.305846 | Train Acc: 66.96480%\n",
      "\t Val. Loss: 0.203365 |  Val. Acc: 80.67540%\n",
      "Epoch: 121 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.293976 | Train Acc: 69.13572%\n",
      "\t Val. Loss: 0.199752 |  Val. Acc: 80.98639%\n",
      "Epoch: 131 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.298264 | Train Acc: 69.02853%\n",
      "\t Val. Loss: 0.208292 |  Val. Acc: 80.22944%\n",
      "Epoch: 141 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.280637 | Train Acc: 71.54561%\n",
      "\t Val. Loss: 0.199586 |  Val. Acc: 81.02118%\n",
      "Epoch: 151 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.276234 | Train Acc: 71.19660%\n",
      "\t Val. Loss: 0.187859 |  Val. Acc: 82.20721%\n",
      "Epoch: 161 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.281203 | Train Acc: 70.29610%\n",
      "\t Val. Loss: 0.188166 |  Val. Acc: 82.18670%\n",
      "Epoch: 171 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.225761 | Train Acc: 76.62935%\n",
      "\t Val. Loss: 0.194858 |  Val. Acc: 81.64981%\n",
      "Epoch: 181 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.263491 | Train Acc: 72.35577%\n",
      "\t Val. Loss: 0.178460 |  Val. Acc: 83.17601%\n",
      "Epoch: 191 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.262735 | Train Acc: 72.20061%\n",
      "\t Val. Loss: 0.202944 |  Val. Acc: 80.83211%\n",
      "Epoch: 201 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.268520 | Train Acc: 71.56261%\n",
      "\t Val. Loss: 0.197721 |  Val. Acc: 81.07495%\n",
      "Epoch: 211 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.243181 | Train Acc: 74.41588%\n",
      "\t Val. Loss: 0.208190 |  Val. Acc: 80.01766%\n",
      "Epoch: 221 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.255974 | Train Acc: 72.34779%\n",
      "\t Val. Loss: 0.191493 |  Val. Acc: 81.57127%\n",
      "Epoch: 231 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.257896 | Train Acc: 72.84250%\n",
      "\t Val. Loss: 0.196094 |  Val. Acc: 81.11859%\n",
      "Epoch: 241 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.257997 | Train Acc: 72.18056%\n",
      "\t Val. Loss: 0.196402 |  Val. Acc: 81.09242%\n",
      "Epoch: 251 | Epoch Time: 0.0m 0.14s\n",
      "\tTrain Loss: 0.238754 | Train Acc: 74.93385%\n",
      "\t Val. Loss: 0.187571 |  Val. Acc: 81.94811%\n",
      "Epoch: 261 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.273065 | Train Acc: 71.54534%\n",
      "\t Val. Loss: 0.196272 |  Val. Acc: 81.22024%\n",
      "Epoch: 271 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.292545 | Train Acc: 68.80010%\n",
      "\t Val. Loss: 0.178200 |  Val. Acc: 82.86062%\n",
      "Epoch: 281 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.274175 | Train Acc: 71.74420%\n",
      "\t Val. Loss: 0.190631 |  Val. Acc: 81.70248%\n",
      "Epoch: 291 | Epoch Time: 0.0m 0.13s\n",
      "\tTrain Loss: 0.249251 | Train Acc: 73.99973%\n",
      "\t Val. Loss: 0.198360 |  Val. Acc: 80.98929%\n",
      "Epoch: 300 | Epoch Time: 0.0m 0.10s\n",
      "\tTrain Loss: 0.251324 | Train Acc: 73.81411%\n",
      "\t Val. Loss: 0.170829 |  Val. Acc: 83.53789%\n"
     ]
    }
   ],
   "source": [
    "# built LSTM model\n",
    "rnn = LongShortTermMemory(time_step, n_features, hidden_size=300, n_layers=2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criteon = nn.MSELoss()\n",
    "\n",
    "# 'R2 score'\n",
    "def binary_acc(preds, y):\n",
    "    u = torch.sum(torch.square(y - preds))\n",
    "    v = torch.sum(torch.square(y - torch.mean(y)))\n",
    "    return 1 - (u/v)\n",
    "\n",
    "\n",
    "def train(rnn, iterator, optimizer, criteon):\n",
    "\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.train() \n",
    "\n",
    "    for batch_id, batch_data in enumerate(iterator):\n",
    "        inputs, labels = batch_data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred, _ = rnn(inputs)\n",
    "#         print(pred.shape, labels.shape)\n",
    "        loss = criteon(pred.squeeze(), labels.squeeze())\n",
    "        acc = binary_acc(pred.squeeze(), labels.squeeze()).item()\n",
    "\n",
    "        avg_loss.append(loss.item())\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate(rnn, iterator, criteon):\n",
    "\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.eval()\n",
    "    att_weights = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in iterator:\n",
    "            inputs, labels = batch_data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred, attention_weight = rnn(inputs)\n",
    "            att_weights.append(attention_weight.squeeze())\n",
    "            loss = criteon(pred.squeeze(), labels.squeeze())\n",
    "            acc = binary_acc(pred, labels).item()\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            avg_acc.append(acc)\n",
    "\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_loss, avg_acc, att_weights\n",
    "\n",
    "\n",
    "#train and print model\n",
    "best_valid_acc = float('-inf')\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(rnn, train_loader, optimizer, criteon)\n",
    "    dev_loss, dev_acc, att_weights = evaluate(rnn, test_loader, criteon)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if dev_acc > best_valid_acc:          #只要模型效果变好，就保存\n",
    "        best_valid_acc = dev_acc\n",
    "        torch.save(rnn.state_dict(), 'lstm-model.pt')\n",
    "    if epoch % 10 == 0 or epoch == epochs-1:\n",
    "        print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.6f} | Train Acc: {train_acc*100:.5f}%')\n",
    "        print(f'\\t Val. Loss: {dev_loss:.6f} |  Val. Acc: {dev_acc*100:.5f}%')\n",
    "\n",
    "\n",
    "# #use srored model to predice \n",
    "# rnn.load_state_dict(torch.load(\"lstm-model.pt\"))\n",
    "# test_loss, test_acc, att_weights = evaluate(rnn, test_iterator, criteon)\n",
    "# print(f'Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc, att_weights = evaluate(rnn, test_loader, criteon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentiment_24h</td>\n",
       "      <td>333.603882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DXYUSD_Gain_Rate</td>\n",
       "      <td>85.545662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TNXTbill_Gain_Rate</td>\n",
       "      <td>10.288287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VIX_Gain_Rate</td>\n",
       "      <td>9.721036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World_Index_Gain_Rate</td>\n",
       "      <td>9.524199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPY_Gain_Rate</td>\n",
       "      <td>8.597301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrudeOil_Gain_Rate</td>\n",
       "      <td>6.869552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DJCI_Gain_Rate</td>\n",
       "      <td>5.096609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DBCCommodity_Gain_Rate</td>\n",
       "      <td>3.469755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Silver_Gain_Rate</td>\n",
       "      <td>2.230522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GoldUSD_Gain_Rate</td>\n",
       "      <td>2.053216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variable  coefficient\n",
       "0            Sentiment_24h   333.603882\n",
       "1         DXYUSD_Gain_Rate    85.545662\n",
       "10      TNXTbill_Gain_Rate    10.288287\n",
       "9            VIX_Gain_Rate     9.721036\n",
       "2    World_Index_Gain_Rate     9.524199\n",
       "8            SPY_Gain_Rate     8.597301\n",
       "7       CrudeOil_Gain_Rate     6.869552\n",
       "6           DJCI_Gain_Rate     5.096609\n",
       "5   DBCCommodity_Gain_Rate     3.469755\n",
       "4         Silver_Gain_Rate     2.230522\n",
       "3        GoldUSD_Gain_Rate     2.053216"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = torch.cat(att_weights, dim=0).cpu()\n",
    "pd.DataFrame(att.numpy().sum(axis=0).reshape(-1, 1))\n",
    "df_coef = pd.DataFrame(columns=['variable', 'coefficient'])\n",
    "df_coef['variable'] = df.columns[:-1]\n",
    "df_coef['coefficient'] = att.numpy().sum(axis=0)\n",
    "# df_coef['abs_coefficient'] = df_coef['coefficient'].abs()\n",
    "# sort by absolute value of coefficient\n",
    "pd.set_option('display.max_rows',None)\n",
    "df_coef.sort_values(by=['coefficient'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
